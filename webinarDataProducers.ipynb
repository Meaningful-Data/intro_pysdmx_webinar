{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Overview of the use case",
   "id": "a32e2eb94c19ca5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---------(Overview on the use case,\n",
    "why is it important for data producers,\n",
    "usefulness of the library\n",
    "when reading non-SDMX data\n",
    "and convert to SDMX. Mention validations and calculations over VTL.)---------"
   ],
   "id": "11302234f134bc8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Talking points and agenda:\n",
    "\n",
    "- General use of pysdmx on Data Producers\n",
    "- Outside the SDMX garden, looking at LEI and GLEIF\n",
    "- Data cleaning and set up using pandas\n",
    "- Downloading and reading the ConceptScheme on SDMX-ML 2.1 using read_sdmx\n",
    "- Retrieving the Schema from FMR (FusionJSON)\n",
    "- Convert the Schema to a VTL DataStructure\n",
    "- Using VTL to validate the data\n",
    "- Using VTL to perform calculations\n",
    "- Generate SDMX file with the aggregated data\n",
    "- Reading back the SDMX file using read_sdmx\n",
    "\n",
    "### List of pysdmx classes and functions used in this notebook:\n",
    "\n",
    "Functions:\n",
    "- pysdmx.io.read_sdmx\n",
    "- pysdmx.io.csv.sdmx20.writer.write\n",
    "\n",
    "Classes:\n",
    "- pysdmx.api.fmr.RegistryClient (and methods)\n",
    "- pysdmx.model.message.Message (and methods)\n",
    "- pysdmx.io.pd.PandasDataset\n",
    "- pysdmx.model.dataflow.Schema"
   ],
   "id": "65d61c815e32a66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Outside the SDMX garden, looking at LEI and GLEIF",
   "id": "5ce654850edb5602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------(Explanation on LEI and GLEIF, use for data producers)---------",
   "id": "2c8795d1c0a1dfe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data cleaning and set up using pandas and pysdmx",
   "id": "f6f0ec891c33d0c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this use case,\n",
    "we will use the Golden Copy file from GLEIF (link)\n",
    "and filter on those LEI with status Active. \n",
    "\n",
    "The code renames the columns and select the data we need for later validation.\n",
    "We added the possibility of saving the data into plain CSV or SDMX-CSV 2.0\n",
    "(using pysdmx).\n",
    "\n",
    "The code uses the chunking capabilities of Pandas for better memory efficiency.\n",
    "This is a prototype of the streaming capabilities with pandas in pysdmx,\n",
    "which will be available by the end of 2025.\n",
    "\n",
    "For the sake of this example,\n",
    "we distinguish between the Golden Copy original path\n",
    "(link to download the file) and the Golden Copy Changed,\n",
    "which would be the output of this code.\n",
    "\n",
    "This code requires to install the extra data from pysdmx,\n",
    "which simply install pandas.\n",
    "\n",
    "```bash\n",
    "pip install pysdmx[data]\n",
    "```"
   ],
   "id": "a62e158b6618a05e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pysdmx.io.csv.sdmx20.writer import write\n",
    "from pysdmx.io.pd import PandasDataset\n",
    "\n",
    "# Original columns and their simple name for next steps of this tutorial\n",
    "RENAME_DICT = {\n",
    "    \"LEI\": \"LEI\",\n",
    "    \"Entity.LegalName\": \"LEGAL_NAME\",\n",
    "    \"Entity.LegalAddress.Country\": \"COUNTRY_INCORPORATION\",\n",
    "    \"Entity.HeadquartersAddress.Country\": \"COUNTRY_HEADQUARTERS\",\n",
    "    \"Entity.EntityCategory\": \"CATEGORY\",\n",
    "    \"Entity.EntitySubCategory\": \"SUBCATEGORY\",\n",
    "    \"Entity.LegalForm.EntityLegalFormCode\": \"LEGAL_FORM\",\n",
    "    \"Entity.EntityStatus\": \"STATUS\",\n",
    "    \"Entity.LegalAddress.PostalCode\": \"POSTAL_CODE\",\n",
    "}\n",
    "\n",
    "\n",
    "def _process_chunk(data: pd.DataFrame):\n",
    "    data.rename(columns=RENAME_DICT, inplace=True)\n",
    "    data = data[list(RENAME_DICT.values())]\n",
    "    data = data[data[\"STATUS\"] == \"ACTIVE\"]\n",
    "    del data[\"STATUS\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def _save_as_sdmx_csv(data: pd.DataFrame):\n",
    "    dataset = PandasDataset(\n",
    "        structure=\"DataStructure=MD:LEI_DATA(1.0)\", data=data\n",
    "    )\n",
    "    return write([dataset])\n",
    "\n",
    "\n",
    "def __clean_output(output, header=False):\n",
    "    \"\"\"Currently may add some extra lines in windows, \n",
    "    just removing the  CR character.\n",
    "    We also clean the extra headers for chunking.\"\"\"\n",
    "    out_lst = output.splitlines()\n",
    "    if not header:\n",
    "        out_lst = out_lst[1:]\n",
    "    output = \"\\n\".join(out_lst)\n",
    "    del out_lst\n",
    "    return output\n",
    "\n",
    "\n",
    "def streaming_load_save_csv_file(golden_copy_original_path, output_filename,\n",
    "                                 use_sdmx_csv=False, nrows=None):\n",
    "    \"\"\"Load data and rename using small memory\"\"\"\n",
    "    chunksize = None\n",
    "    if nrows is None or nrows > 100000:\n",
    "        chunksize = 100000\n",
    "    data = pd.read_csv(golden_copy_original_path, dtype=str,\n",
    "                       chunksize=chunksize, nrows=nrows)\n",
    "    # Add header only to the first chunk\n",
    "    add_header = True\n",
    "    # Removing the file if already present\n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "    number_of_lines_written = 0\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = [data]\n",
    "    for chunk in data:\n",
    "        chunk = _process_chunk(chunk)\n",
    "        if add_header:\n",
    "            header = True\n",
    "            add_header = False\n",
    "        else:\n",
    "            header = False\n",
    "        number_of_lines_written += len(chunk)\n",
    "        if not use_sdmx_csv:\n",
    "            chunk.to_csv(output_filename, mode=\"a\", index=False, header=header)\n",
    "        else:\n",
    "            out = _save_as_sdmx_csv(chunk)\n",
    "            out = __clean_output(out, header)\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(out)\n",
    "    print(f\"Number of lines written: {number_of_lines_written}\")\n",
    "\n",
    "\n",
    "streaming_load_save_csv_file(\n",
    "    golden_copy_original_path=\"data_files/golden-copy-original.csv\",\n",
    "    output_filename=\"data_files/golden_copy_changed_10000_sdmx.csv\",\n",
    "    use_sdmx_csv=True, nrows=10000)"
   ],
   "id": "2efc7983a637dd91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reading the ConceptScheme on SDMX-ML 2.1 using read_sdmx",
   "id": "e9cd54706832df80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this example,\n",
    "we generated a DataStructure on FMR called LEI_DATA,\n",
    "with Short URN: DataStructure=MD:LEI_DATA(1.0),\n",
    "with the required codelists to be used for structural validation on FMR.\n",
    "\n",
    "This structures is also available at SDMX_Structures/structures.xml file in this project,\n",
    "or at the MeaningfulData FMR (fmr.meaningfuldata.eu).\n",
    "Currently the library does not support SDMX-ML 3.0,\n",
    "so we will read only the ConceptScheme and descendants (available at SDMX_Structures/concepts.xml).\n",
    "\n",
    "To ensure we are able to validate the data correctly,\n",
    "we extended the CL_AREA codelist from SDMX\n",
    "to add a code that was present in the LEI Golden Copy.\n",
    "\n",
    "The code below reads the ConceptScheme and descendants,\n",
    "ensure you have installed the xml extra from pysdmx.\n",
    "\n",
    "```bash\n",
    "pip install pysdmx[xml]\n",
    "```"
   ],
   "id": "ba71c4a3cf16ae61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysdmx.io import read_sdmx\n",
    "\n",
    "structures_msg = read_sdmx(\"SDMX_Structures/concepts.xml\")\n",
    "# We can access the first concept scheme, or look for the short_urn\n",
    "concept_scheme1 = structures_msg.get_concept_schemes()[0]\n",
    "concept_scheme2 = structures_msg.get_concept_scheme(\n",
    "    \"ConceptScheme=MD:LEI_CONCEPTS(1.0)\")"
   ],
   "id": "54730d5e00a754a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieving the Schema from FMR",
   "id": "ee14d288997e4000"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We may use as well the FMR Webservices to download the Schema from FMR, using the FusionJSON format.",
   "id": "d713bc0ad0a4400c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "18c8b5283c7d6919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from pysdmx.api.fmr import RegistryClient\n",
    "from pysdmx.io.format import StructureFormat\n",
    "\n",
    "client = RegistryClient(\n",
    "    \"https://fmr.meaningfuldata.eu/sdmx/v2\", format=StructureFormat.FUSION_JSON\n",
    ")\n",
    "# Recommend to use debugger to see the response\n",
    "schema = client.get_schema(\n",
    "    \"datastructure\", agency=\"MD\", id=\"LEI_DATA\", version=\"1.0\"\n",
    ")\n",
    "pprint(schema)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using VTL to validate the data with GLEIF data quality checks",
   "id": "3f3374bb987f74c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The VTL language allows us to perform validations over the data,\n",
    "with a business friendly syntax. \n",
    "\n",
    "For this purpose, at MeaningfulData we have developed a library called vtlengine,\n",
    "which is able to run VTL scripts over data.\n",
    "\n",
    "In this example,\n",
    "we will use a VTL script\n",
    "that performs validations based on the GLEIF data quality checks\n",
    "(link) and a custom validation on Subcategory data.\n",
    "\n",
    "\n",
    "Steps to use VTL from pysdmx:\n",
    "1. Convert the Schema to a VTL DataStructure\n",
    "2. Validate the data using VTL\n",
    "3. Analyse the results\n",
    "\n",
    "---------(Explanation on VTL validations usefulness,\n",
    "validating more than one component. Quick overview on the code\n",
    "using VTL Playground)---------"
   ],
   "id": "946c133de69a530"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert the Schema to a VTL DataStructure",
   "id": "c338d2e18812b045"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code converts the pysdmx.model Schema and DataStructureDefinition objects into a VTL datastructure,\n",
    "using MeaningfulData internal format, usable only with vtlengine.\n",
    "On pysdmx we will include this method\n",
    "but it will generate the VTL 2.1 Standard datastructure.\n",
    "Both options will be usable by the vtlengine library."
   ],
   "id": "163d8b1bb245a5c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up the code",
   "id": "ccfdcbf55e93e644"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "from pysdmx.model.dataflow import DataStructureDefinition, Component\n",
    "from pysdmx.model import Role\n",
    "\n",
    "VTL_DTYPES_MAPPING = {\n",
    "    \"String\": \"String\",\n",
    "    \"Alpha\": \"String\",\n",
    "    \"AlphaNumeric\": \"String\",\n",
    "    \"Numeric\": \"String\",\n",
    "    \"BigInteger\": \"Integer\",\n",
    "    \"Integer\": \"Integer\",\n",
    "    \"Long\": \"Integer\",\n",
    "    \"Short\": \"Integer\",\n",
    "    \"Decimal\": \"Number\",\n",
    "    \"Float\": \"Number\",\n",
    "    \"Double\": \"Number\",\n",
    "    \"Boolean\": \"Boolean\",\n",
    "    \"URI\": \"String\",\n",
    "    \"Count\": \"Integer\",\n",
    "    \"InclusiveValueRange\": \"Number\",\n",
    "    \"ExclusiveValueRange\": \"Number\",\n",
    "    \"Incremental\": \"Number\",\n",
    "    \"ObservationalTimePeriod\": \"Time_Period\",\n",
    "    \"StandardTimePeriod\": \"Time_Period\",\n",
    "    \"BasicTimePeriod\": \"Date\",\n",
    "    \"GregorianTimePeriod\": \"Date\",\n",
    "    \"GregorianYear\": \"Date\",\n",
    "    \"GregorianYearMonth\": \"Date\",\n",
    "    \"GregorianMonth\": \"Date\",\n",
    "    \"GregorianDay\": \"Date\",\n",
    "    \"ReportingTimePeriod\": \"Time_Period\",\n",
    "    \"ReportingYear\": \"Time_Period\",\n",
    "    \"ReportingSemester\": \"Time_Period\",\n",
    "    \"ReportingTrimester\": \"Time_Period\",\n",
    "    \"ReportingQuarter\": \"Time_Period\",\n",
    "    \"ReportingMonth\": \"Time_Period\",\n",
    "    \"ReportingWeek\": \"Time_Period\",\n",
    "    \"ReportingDay\": \"Time_Period\",\n",
    "    \"DateTime\": \"Date\",\n",
    "    \"TimeRange\": \"Time\",\n",
    "    \"Month\": \"String\",\n",
    "    \"MonthDay\": \"String\",\n",
    "    \"Day\": \"String\",\n",
    "    \"Time\": \"String\",\n",
    "    \"Duration\": \"Duration\",\n",
    "}\n",
    "\n",
    "VTL_ROLE_MAPPING = {\n",
    "    Role.DIMENSION: \"Identifier\",\n",
    "    Role.MEASURE: \"Measure\",\n",
    "    Role.ATTRIBUTE: \"Attribute\",\n",
    "}\n",
    "\n",
    "\n",
    "def to_vtl_json(\n",
    "        dsd: DataStructureDefinition, path: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Formats the DataStructureDefinition as a VTL DataStructure.\"\"\"\n",
    "    dataset_name = dsd.id\n",
    "    components = []\n",
    "    NAME = \"name\"\n",
    "    ROLE = \"role\"\n",
    "    TYPE = \"type\"\n",
    "    NULLABLE = \"nullable\"\n",
    "\n",
    "    _components: List[Component] = []\n",
    "    _components.extend(dsd.components.dimensions)\n",
    "    _components.extend(dsd.components.measures)\n",
    "    _components.extend(dsd.components.attributes)\n",
    "\n",
    "    for c in _components:\n",
    "        _type = VTL_DTYPES_MAPPING[c.dtype]\n",
    "        _nullability = c.role != Role.DIMENSION\n",
    "        _role = VTL_ROLE_MAPPING[c.role]\n",
    "\n",
    "        component = {\n",
    "            NAME: c.id,\n",
    "            ROLE: _role,\n",
    "            TYPE: _type,\n",
    "            NULLABLE: _nullability,\n",
    "        }\n",
    "\n",
    "        components.append(component)\n",
    "\n",
    "    result = {\n",
    "        \"datasets\": [{\"name\": dataset_name, \"DataStructure\": components}]\n",
    "    }\n",
    "    if path is not None:\n",
    "        with open(path, \"w\") as fp:\n",
    "            json.dump(result, fp, indent=2)\n",
    "        return None\n",
    "\n",
    "    return result"
   ],
   "id": "bb15dbe1bbae6074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Perform the conversion",
   "id": "43016438c567768c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "vtl_datastructure = to_vtl_json(schema)\n",
    "pprint(vtl_datastructure)"
   ],
   "id": "47d0aa10345373df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fb8227757fc3f412"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate the data using VTL (sample 10000)",
   "id": "27033a888adf030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting up the code",
   "id": "4da0b6b4d28ca7da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from vtlengine import run\n",
    "\n",
    "\n",
    "def _load_script(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        script = f.read()\n",
    "    return script"
   ],
   "id": "dd946eb30616af39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------(Explanation on the code, overview on the VTL run method documentation)---------",
   "id": "15e00e96a0d8eb68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running the VTL script",
   "id": "42b6c4a5fbb1cefc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "script = _load_script(\"vtl/script.vtl\")\n",
    "data_df = pd.read_csv(\"data_files/golden_copy_changed_10000.csv\")\n",
    "datapoints = {\"LEI_DATA\": data_df}\n",
    "\n",
    "run_result = run(script=script, data_structures=vtl_datastructure,\n",
    "                 datapoints=datapoints)\n",
    "pprint(run_result)"
   ],
   "id": "2f848e828a5a3ccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting the total number of errors (sample 10000)",
   "id": "711d3b08dba1ffe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_result['errors_count'].data",
   "id": "8637c1a26593e8d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysing data on Subcategory errors (sample 10000)",
   "id": "63e6c09dae75a208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols_to_analyse = ['CATEGORY', 'SUBCATEGORY', 'errorcode', 'errorlevel']\n",
    "run_result['validation.subcategories_errors'].data[cols_to_analyse]"
   ],
   "id": "c313bba6c3c7e2fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------(Explanation on Subcategory errors)---------",
   "id": "92941fb6751112c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ",
   "id": "8ea685c47091461f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "146e83a27074f54",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
