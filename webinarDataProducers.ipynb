{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32e2eb94c19ca5d",
   "metadata": {},
   "source": [
    "# Overview of the use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11302234f134bc8a",
   "metadata": {},
   "source": [
    "---------(Overview on the use case,\n",
    "why is it important for data producers,\n",
    "usefulness of the library\n",
    "when reading non-SDMX data\n",
    "and convert to SDMX. Mention validations and calculations over VTL.)---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d61c815e32a66",
   "metadata": {},
   "source": [
    "Talking points and agenda:\n",
    "\n",
    "- General use of pysdmx on Data Producers\n",
    "- Outside the SDMX garden, looking at LEI and GLEIF\n",
    "- Data cleaning and set up using pandas\n",
    "- Downloading and reading the ConceptScheme on SDMX-ML 2.1 using read_sdmx\n",
    "- Retrieving the Schema from FMR (FusionJSON)\n",
    "- Convert the Schema to a VTL DataStructure\n",
    "- Using VTL to validate the data\n",
    "- Using VTL to perform calculations\n",
    "- Generate SDMX file with the aggregated data\n",
    "- Reading back the SDMX file using read_sdmx\n",
    "\n",
    "### List of pysdmx classes and functions used in this notebook:\n",
    "\n",
    "Functions:\n",
    "- pysdmx.io.read_sdmx\n",
    "- pysdmx.io.csv.sdmx20.writer.write\n",
    "\n",
    "Classes:\n",
    "- pysdmx.api.fmr.RegistryClient (and methods)\n",
    "- pysdmx.model.message.Message (and methods)\n",
    "- pysdmx.io.pd.PandasDataset\n",
    "- pysdmx.model.dataflow.Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce654850edb5602",
   "metadata": {},
   "source": [
    "# Outside the SDMX garden, looking at LEI and GLEIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8795d1c0a1dfe4",
   "metadata": {},
   "source": [
    "---------(Explanation on LEI and GLEIF, use for data producers)---------\n",
    "\n",
    "The Legal Entity Identifier (LEI) is a unique global identifier for legal entities participating in financial transactions. Its purpose is to help identify legal entities on a globally accessible database (source [Wikipedia](https://en.wikipedia.org/wiki/Legal_Entity_Identifier).\n",
    "\n",
    "The Global LEI Foundation (GLEIF) supports the implementation and use of the LEI. It makes it possible to access all the LEI records through APIs or downloading file.\n",
    "\n",
    "The GLEIF site does not use SDMX. It maintains a [data dictionary](https://www.gleif.org/en/lei-data/access-and-use-lei-data/gleif-data-dictionary) and [an API](https://www.gleif.org/en/lei-data/gleif-api). It also makes possible downloading [golden copies of the data, as well as delta files](https://www.gleif.org/en/lei-data/gleif-golden-copy).\n",
    "\n",
    "\n",
    "As relevant master data about entities, a good integration with the GLEIF data may be in the interest of many institutions.\n",
    "For institutions having SDMX-driven system, it may be useful to create SDMX metadata and convert the LEI data from their source formats to SDMX, so that they can be integrated int their systems. Besides, it may be useful for those entities to validate the input data an generate new statistics.\n",
    "\n",
    "This notebook is presenting a way to use pysdmx and the VTL Engine to represent a statistical process that includes:\n",
    "1. The colletion of data from the GLEIF\n",
    "2. The transformation of data into SDMX\n",
    "3. The structural validation of data using FMR\n",
    "4. The consistency validation of the data using VTL\n",
    "5. The generation of new aggregated statistics using VTL\n",
    "6. The conversion of the data into SDMX\n",
    "\n",
    "\n",
    "For this exercise, the necessary SDMX metadata have been added to an FMR instance hosted by Meaningfuldata (https://fmr.meaningfuldata.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0ec891c33d0c2",
   "metadata": {},
   "source": [
    "## Data cleaning and set up using pandas and pysdmx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e158b6618a05e",
   "metadata": {},
   "source": [
    "For this excercise, we will use as input the [golden copy from the GLEIF](https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/), and we will transform it to get a dataset following the desired data structure, which can be fournd [here](https://fmr.meaningfuldata.eu/sdmx/v2/structure/datastructure/MD/LEI_DATA/1.0).\n",
    "\n",
    "Note that we designed this DSD from the existing data, but took a subset of the data and renamed the attributes to make them closer to SDMX practices.\n",
    "\n",
    "We are using Pandas to read the original data and to transform them to get a final dataset that follows the DSD.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Downoload the data from [the source](https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#.zip)\n",
    "2. Read the downloaded data wiht Pandas\n",
    "3. Drop the columns not used in the DSD and rename the existing ones\n",
    "4. Filter to get only the active entities (the GLEIF publishes also inactive entities)\n",
    "\n",
    "The code uses the chunking capabilities of Pandas for better memory efficiency.\n",
    "This is a prototype of the streaming capabilities with pandas in pysdmx,\n",
    "which will be available by the end of 2025.\n",
    "\n",
    "This code requires to install the extra data from pysdmx,\n",
    "which simply install pandas.\n",
    "\n",
    "```bash\n",
    "pip install pysdmx[data]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e1e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "from pysdmx.io.csv.sdmx20.writer import write\n",
    "from pysdmx.io.pd import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81302d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Download the Golden Copy file\n",
    "\n",
    "GOLDEN_COPY_PATH = 'data_files/lei_golden_copy'\n",
    "\n",
    "url = 'https://leidata-preview.gleif.org/storage/golden-copy-files/2025/01/25/1034569/20250125-1600-gleif-goldencopy-lei2-golden-copy.csv.zip'\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(GOLDEN_COPY_PATH + '.zip', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(GOLDEN_COPY_PATH + '.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data_files/')\n",
    "    file_name = zip_ref.namelist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read the Golden Copy file\n",
    "data = pd.read_csv('data_files/' + file_name, dtype=str, chunksize = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09566b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rename and keep columns\n",
    "\n",
    "# Original columns and their simple name for next steps of this tutorial\n",
    "RENAME_DICT = {\n",
    "    \"LEI\": \"LEI\",\n",
    "    \"Entity.LegalName\": \"LEGAL_NAME\",\n",
    "    \"Entity.LegalAddress.Country\": \"COUNTRY_INCORPORATION\",\n",
    "    \"Entity.HeadquartersAddress.Country\": \"COUNTRY_HEADQUARTERS\",\n",
    "    \"Entity.EntityCategory\": \"CATEGORY\",\n",
    "    \"Entity.EntitySubCategory\": \"SUBCATEGORY\",\n",
    "    \"Entity.LegalForm.EntityLegalFormCode\": \"LEGAL_FORM\",\n",
    "    \"Entity.EntityStatus\": \"STATUS\",\n",
    "    \"Entity.LegalAddress.PostalCode\": \"POSTAL_CODE\",\n",
    "}\n",
    "\n",
    "data = data[list(RENAME_DICT.keys())]\n",
    "data.rename(columns=RENAME_DICT, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Filter to get only the required columns\n",
    "data = data[data[\"STATUS\"] == \"ACTIVE\"]\n",
    "del data[\"STATUS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efc7983a637dd91",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_files/golden-copy-original.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 82\u001b[0m\n\u001b[0;32m     78\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(out)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of lines written: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_lines_written\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m \u001b[43mstreaming_load_save_csv_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgolden_copy_original_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_files/golden-copy-original.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_files/golden_copy_changed_10000_sdmx.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sdmx_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mstreaming_load_save_csv_file\u001b[1;34m(golden_copy_original_path, output_filename, use_sdmx_csv, nrows)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nrows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m nrows \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100000\u001b[39m:\n\u001b[0;32m     53\u001b[0m     chunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m---> 54\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgolden_copy_original_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Add header only to the first chunk\u001b[39;00m\n\u001b[0;32m     57\u001b[0m add_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aolle\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\intro-pysdmx-webinar-rhr90ZG6-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aolle\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\intro-pysdmx-webinar-rhr90ZG6-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\aolle\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\intro-pysdmx-webinar-rhr90ZG6-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aolle\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\intro-pysdmx-webinar-rhr90ZG6-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\aolle\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\intro-pysdmx-webinar-rhr90ZG6-py3.11\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_files/golden-copy-original.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Original columns and their simple name for next steps of this tutorial\n",
    "RENAME_DICT = {\n",
    "    \"LEI\": \"LEI\",\n",
    "    \"Entity.LegalName\": \"LEGAL_NAME\",\n",
    "    \"Entity.LegalAddress.Country\": \"COUNTRY_INCORPORATION\",\n",
    "    \"Entity.HeadquartersAddress.Country\": \"COUNTRY_HEADQUARTERS\",\n",
    "    \"Entity.EntityCategory\": \"CATEGORY\",\n",
    "    \"Entity.EntitySubCategory\": \"SUBCATEGORY\",\n",
    "    \"Entity.LegalForm.EntityLegalFormCode\": \"LEGAL_FORM\",\n",
    "    \"Entity.EntityStatus\": \"STATUS\",\n",
    "    \"Entity.LegalAddress.PostalCode\": \"POSTAL_CODE\",\n",
    "}\n",
    "\n",
    "\n",
    "def _process_chunk(data: pd.DataFrame):\n",
    "    data.rename(columns=RENAME_DICT, inplace=True)\n",
    "    data = data[list(RENAME_DICT.values())]\n",
    "    data = data[data[\"STATUS\"] == \"ACTIVE\"]\n",
    "    del data[\"STATUS\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def _save_as_sdmx_csv(data: pd.DataFrame):\n",
    "    dataset = PandasDataset(\n",
    "        structure=\"DataStructure=MD:LEI_DATA(1.0)\", data=data\n",
    "    )\n",
    "    return write([dataset])\n",
    "\n",
    "\n",
    "def __clean_output(output, header=False):\n",
    "    \"\"\"Currently may add some extra lines in windows, \n",
    "    just removing the  CR character.\n",
    "    We also clean the extra headers for chunking.\"\"\"\n",
    "    out_lst = output.splitlines()\n",
    "    if not header:\n",
    "        out_lst = out_lst[1:]\n",
    "    output = \"\\n\".join(out_lst)\n",
    "    del out_lst\n",
    "    return output\n",
    "\n",
    "\n",
    "def streaming_load_save_csv_file(golden_copy_original_path, output_filename,\n",
    "                                 use_sdmx_csv=False, nrows=None):\n",
    "    \"\"\"Load data and rename using small memory\"\"\"\n",
    "    chunksize = None\n",
    "    if nrows is None or nrows > 100000:\n",
    "        chunksize = 100000\n",
    "    data = pd.read_csv(golden_copy_original_path, dtype=str,\n",
    "                       chunksize=chunksize, nrows=nrows)\n",
    "    # Add header only to the first chunk\n",
    "    add_header = True\n",
    "    # Removing the file if already present\n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "    number_of_lines_written = 0\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = [data]\n",
    "    for chunk in data:\n",
    "        chunk = _process_chunk(chunk)\n",
    "        if add_header:\n",
    "            header = True\n",
    "            add_header = False\n",
    "        else:\n",
    "            header = False\n",
    "        number_of_lines_written += len(chunk)\n",
    "        if not use_sdmx_csv:\n",
    "            chunk.to_csv(output_filename, mode=\"a\", index=False, header=header)\n",
    "        else:\n",
    "            out = _save_as_sdmx_csv(chunk)\n",
    "            out = __clean_output(out, header)\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(out)\n",
    "    print(f\"Number of lines written: {number_of_lines_written}\")\n",
    "\n",
    "\n",
    "streaming_load_save_csv_file(\n",
    "    golden_copy_original_path=\"data_files/golden-copy-original.csv\",\n",
    "    output_filename=\"data_files/golden_copy_changed_10000_sdmx.csv\",\n",
    "    use_sdmx_csv=True, nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd54706832df80",
   "metadata": {},
   "source": [
    "# Reading the ConceptScheme on SDMX-ML 2.1 using read_sdmx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71c4a3cf16ae61",
   "metadata": {},
   "source": [
    "For this example,\n",
    "we generated a DataStructure on FMR called LEI_DATA,\n",
    "with Short URN: DataStructure=MD:LEI_DATA(1.0),\n",
    "with the required codelists to be used for structural validation on FMR.\n",
    "\n",
    "This structures is also available at SDMX_Structures/structures.xml file in this project,\n",
    "or at the MeaningfulData FMR (fmr.meaningfuldata.eu).\n",
    "Currently the library does not support SDMX-ML 3.0,\n",
    "so we will read only the ConceptScheme and descendants (available at SDMX_Structures/concepts.xml).\n",
    "\n",
    "To ensure we are able to validate the data correctly,\n",
    "we extended the CL_AREA codelist from SDMX\n",
    "to add a code that was present in the LEI Golden Copy.\n",
    "\n",
    "The code below reads the ConceptScheme and descendants,\n",
    "ensure you have installed the xml extra from pysdmx.\n",
    "\n",
    "```bash\n",
    "pip install pysdmx[xml]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54730d5e00a754a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysdmx.io import read_sdmx\n",
    "\n",
    "structures_msg = read_sdmx(\"SDMX_Structures/concepts.xml\")\n",
    "# We can access the first concept scheme, or look for the short_urn\n",
    "concept_scheme1 = structures_msg.get_concept_schemes()[0]\n",
    "concept_scheme2 = structures_msg.get_concept_scheme(\n",
    "    \"ConceptScheme=MD:LEI_CONCEPTS(1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14d288997e4000",
   "metadata": {},
   "source": [
    "# Retrieving the Schema from FMR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713bc0ad0a4400c",
   "metadata": {},
   "source": [
    "We may use as well the FMR Webservices to download the Schema from FMR, using the FusionJSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8b5283c7d6919",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pysdmx.api.fmr import RegistryClient\n",
    "from pysdmx.io.format import StructureFormat\n",
    "\n",
    "client = RegistryClient(\n",
    "    \"https://fmr.meaningfuldata.eu/sdmx/v2\", format=StructureFormat.FUSION_JSON\n",
    ")\n",
    "# Recommend to use debugger to see the response\n",
    "schema = client.get_schema(\n",
    "    \"datastructure\", agency=\"MD\", id=\"LEI_DATA\", version=\"1.0\"\n",
    ")\n",
    "pprint(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3374bb987f74c8",
   "metadata": {},
   "source": [
    "# Using VTL to validate the data with GLEIF data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c133de69a530",
   "metadata": {},
   "source": [
    "The VTL language allows us to perform validations over the data,\n",
    "with a business friendly syntax. \n",
    "\n",
    "For this purpose, at MeaningfulData we have developed a library called vtlengine,\n",
    "which is able to run VTL scripts over data.\n",
    "\n",
    "In this example,\n",
    "we will use a VTL script\n",
    "that performs validations based on the GLEIF data quality checks\n",
    "(link) and a custom validation on Subcategory data.\n",
    "\n",
    "\n",
    "Steps to use VTL from pysdmx:\n",
    "1. Convert the Schema to a VTL DataStructure\n",
    "2. Validate the data using VTL\n",
    "3. Analyse the results\n",
    "\n",
    "---------(Explanation on VTL validations usefulness,\n",
    "validating more than one component. Quick overview on the code\n",
    "using VTL Playground)---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338d2e18812b045",
   "metadata": {},
   "source": [
    "## Convert the Schema to a VTL DataStructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d8b1bb245a5c0",
   "metadata": {},
   "source": [
    "This code converts the pysdmx.model Schema and DataStructureDefinition objects into a VTL datastructure,\n",
    "using MeaningfulData internal format, usable only with vtlengine.\n",
    "On pysdmx we will include this method\n",
    "but it will generate the VTL 2.1 Standard datastructure.\n",
    "Both options will be usable by the vtlengine library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdcbf55e93e644",
   "metadata": {},
   "source": [
    "## Setting up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb15dbe1bbae6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "from pysdmx.model.dataflow import DataStructureDefinition, Component\n",
    "from pysdmx.model import Role\n",
    "\n",
    "VTL_DTYPES_MAPPING = {\n",
    "    \"String\": \"String\",\n",
    "    \"Alpha\": \"String\",\n",
    "    \"AlphaNumeric\": \"String\",\n",
    "    \"Numeric\": \"String\",\n",
    "    \"BigInteger\": \"Integer\",\n",
    "    \"Integer\": \"Integer\",\n",
    "    \"Long\": \"Integer\",\n",
    "    \"Short\": \"Integer\",\n",
    "    \"Decimal\": \"Number\",\n",
    "    \"Float\": \"Number\",\n",
    "    \"Double\": \"Number\",\n",
    "    \"Boolean\": \"Boolean\",\n",
    "    \"URI\": \"String\",\n",
    "    \"Count\": \"Integer\",\n",
    "    \"InclusiveValueRange\": \"Number\",\n",
    "    \"ExclusiveValueRange\": \"Number\",\n",
    "    \"Incremental\": \"Number\",\n",
    "    \"ObservationalTimePeriod\": \"Time_Period\",\n",
    "    \"StandardTimePeriod\": \"Time_Period\",\n",
    "    \"BasicTimePeriod\": \"Date\",\n",
    "    \"GregorianTimePeriod\": \"Date\",\n",
    "    \"GregorianYear\": \"Date\",\n",
    "    \"GregorianYearMonth\": \"Date\",\n",
    "    \"GregorianMonth\": \"Date\",\n",
    "    \"GregorianDay\": \"Date\",\n",
    "    \"ReportingTimePeriod\": \"Time_Period\",\n",
    "    \"ReportingYear\": \"Time_Period\",\n",
    "    \"ReportingSemester\": \"Time_Period\",\n",
    "    \"ReportingTrimester\": \"Time_Period\",\n",
    "    \"ReportingQuarter\": \"Time_Period\",\n",
    "    \"ReportingMonth\": \"Time_Period\",\n",
    "    \"ReportingWeek\": \"Time_Period\",\n",
    "    \"ReportingDay\": \"Time_Period\",\n",
    "    \"DateTime\": \"Date\",\n",
    "    \"TimeRange\": \"Time\",\n",
    "    \"Month\": \"String\",\n",
    "    \"MonthDay\": \"String\",\n",
    "    \"Day\": \"String\",\n",
    "    \"Time\": \"String\",\n",
    "    \"Duration\": \"Duration\",\n",
    "}\n",
    "\n",
    "VTL_ROLE_MAPPING = {\n",
    "    Role.DIMENSION: \"Identifier\",\n",
    "    Role.MEASURE: \"Measure\",\n",
    "    Role.ATTRIBUTE: \"Attribute\",\n",
    "}\n",
    "\n",
    "\n",
    "def to_vtl_json(\n",
    "        dsd: DataStructureDefinition, path: Optional[str] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Formats the DataStructureDefinition as a VTL DataStructure.\"\"\"\n",
    "    dataset_name = dsd.id\n",
    "    components = []\n",
    "    NAME = \"name\"\n",
    "    ROLE = \"role\"\n",
    "    TYPE = \"type\"\n",
    "    NULLABLE = \"nullable\"\n",
    "\n",
    "    _components: List[Component] = []\n",
    "    _components.extend(dsd.components.dimensions)\n",
    "    _components.extend(dsd.components.measures)\n",
    "    _components.extend(dsd.components.attributes)\n",
    "\n",
    "    for c in _components:\n",
    "        _type = VTL_DTYPES_MAPPING[c.dtype]\n",
    "        _nullability = c.role != Role.DIMENSION\n",
    "        _role = VTL_ROLE_MAPPING[c.role]\n",
    "\n",
    "        component = {\n",
    "            NAME: c.id,\n",
    "            ROLE: _role,\n",
    "            TYPE: _type,\n",
    "            NULLABLE: _nullability,\n",
    "        }\n",
    "\n",
    "        components.append(component)\n",
    "\n",
    "    result = {\n",
    "        \"datasets\": [{\"name\": dataset_name, \"DataStructure\": components}]\n",
    "    }\n",
    "    if path is not None:\n",
    "        with open(path, \"w\") as fp:\n",
    "            json.dump(result, fp, indent=2)\n",
    "        return None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43016438c567768c",
   "metadata": {},
   "source": [
    "## Perform the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0aa10345373df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "vtl_datastructure = to_vtl_json(schema)\n",
    "pprint(vtl_datastructure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8227757fc3f412",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27033a888adf030",
   "metadata": {},
   "source": [
    "## Validate the data using VTL (sample 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0b6b4d28ca7da",
   "metadata": {},
   "source": [
    "## Setting up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd946eb30616af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vtlengine import run\n",
    "\n",
    "\n",
    "def _load_script(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        script = f.read()\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e00e96a0d8eb68",
   "metadata": {},
   "source": [
    "---------(Explanation on the code, overview on the VTL run method documentation)---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6c4a5fbb1cefc",
   "metadata": {},
   "source": [
    "## Running the VTL script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f848e828a5a3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = _load_script(\"vtl/script.vtl\")\n",
    "data_df = pd.read_csv(\"data_files/golden_copy_changed_10000.csv\")\n",
    "datapoints = {\"LEI_DATA\": data_df}\n",
    "\n",
    "run_result = run(script=script, data_structures=vtl_datastructure,\n",
    "                 datapoints=datapoints)\n",
    "pprint(run_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d3b08dba1ffe7",
   "metadata": {},
   "source": [
    "### Getting the total number of errors (sample 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637c1a26593e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result['errors_count'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6c09dae75a208",
   "metadata": {},
   "source": [
    "### Analysing data on Subcategory errors (sample 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313bba6c3c7e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_analyse = ['CATEGORY', 'SUBCATEGORY', 'errorcode', 'errorlevel']\n",
    "run_result['validation.subcategories_errors'].data[cols_to_analyse]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92941fb6751112c7",
   "metadata": {},
   "source": [
    "---------(Explanation on Subcategory errors)---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea685c47091461f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e83a27074f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-pysdmx-webinar-rhr90ZG6-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
